{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Using NLTK"
      ],
      "metadata": {
        "id": "OKHO4hu0XEwT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "I_N0YJ-yW7Cd"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Tokenization is the process of splitting text into tokens. Stemming reduces words to their root form.\""
      ],
      "metadata": {
        "id": "T9Ojs8B0XKxI"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.lower()\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SSp8u0aMXKzv",
        "outputId": "b190fa30-4922-4d8a-f7c9-42e67d47905c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tokenization is the process of splitting text into tokens. stemming reduces words to their root form.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "tokens = [token for token in tokens if token not in string.punctuation]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_cYwGNpXK2r",
        "outputId": "b6f2e2d2-bc63-4957-fe5f-d8fa50828b84"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tokenization', 'is', 'the', 'process', 'of', 'splitting', 'text', 'into', 'tokens', 'stemming', 'reduces', 'words', 'to', 'their', 'root', 'form']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stems = [stemmer.stem(token) for token in tokens]\n",
        "print(stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akddSCjaX8Pi",
        "outputId": "7fd8534a-be60-497d-e1b2-955a9b7688f9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['token', 'is', 'the', 'process', 'of', 'split', 'text', 'into', 'token', 'stem', 'reduc', 'word', 'to', 'their', 'root', 'form']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "print(lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crHj-ZAJXK5N",
        "outputId": "66dfa0ae-5843-4a65-b8ea-7eedfff2edbe"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tokenization', 'is', 'the', 'process', 'of', 'splitting', 'text', 'into', 'token', 'stemming', 'reduces', 'word', 'to', 'their', 'root', 'form']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Hugging Face"
      ],
      "metadata": {
        "id": "DNwAtNsuY8nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import re"
      ],
      "metadata": {
        "id": "fapSY0IBXK8F"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Tokenization is the process of splitting text into tokens. Stemming reduces words to their root form.\""
      ],
      "metadata": {
        "id": "3X_QR8XZXK-8"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.lower()\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCWJbaaHZG4B",
        "outputId": "a5fb83c4-a22a-46d2-e045-33b305d18843"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenization is the process of splitting text into tokens. stemming reduces words to their root form.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove punctuation using regex\n",
        "text = re.sub(r'[^\\w\\s]', '', text)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwpQNqEiZG6m",
        "outputId": "2f03037c-d3fc-4aea-a3ae-7ef38fa69566"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenization is the process of splitting text into tokens stemming reduces words to their root form\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face Tokenization\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokens = tokenizer.tokenize(text)"
      ],
      "metadata": {
        "id": "BJatXa-oZG9N"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stems = [stemmer.stem(token) for token in tokens]\n",
        "print(stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TuHu5FfZG_S",
        "outputId": "2d8e388c-24ae-418f-9fe7-85755e178619"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['token', '##izat', 'is', 'the', 'process', 'of', 'split', 'text', 'into', 'token', '##', 'stem', 'reduc', 'word', 'to', 'their', 'root', 'form']\n"
          ]
        }
      ]
    }
  ]
}