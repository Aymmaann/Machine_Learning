{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Preprocessing Using NLTK"
      ],
      "metadata": {
        "id": "OKHO4hu0XEwT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "I_N0YJ-yW7Cd"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"NLTK is a powerful Python library for natural language processing! It helps with tokenization, stemming, and more.\""
      ],
      "metadata": {
        "id": "T9Ojs8B0XKxI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence tokenization\n",
        "sent_tokens = sent_tokenize(text)\n",
        "print(\"Sentence Tokens:\", sent_tokens)\n",
        "\n",
        "# Word tokenization\n",
        "word_tokens = word_tokenize(text)\n",
        "print(\"Word Tokens:\", word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSp8u0aMXKzv",
        "outputId": "98be9dc0-44dc-40d4-85c9-c9b88c2bce5f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokens: ['NLTK is a powerful Python library for natural language processing!', 'It helps with tokenization, stemming, and more.']\n",
            "Word Tokens: ['NLTK', 'is', 'a', 'powerful', 'Python', 'library', 'for', 'natural', 'language', 'processing', '!', 'It', 'helps', 'with', 'tokenization', ',', 'stemming', ',', 'and', 'more', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in word_tokens if word not in stop_words]\n",
        "print(\"Tokens without Stopwords:\", filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tffeLF5k8uFH",
        "outputId": "781b1bff-ff30-4f49-bbd7-ea74a5e77012"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens without Stopwords: ['NLTK', 'powerful', 'Python', 'library', 'natural', 'language', 'processing', '!', 'It', 'helps', 'tokenization', ',', 'stemming', ',', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming(Reduce words to their root form)\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_tokens]\n",
        "print(\"Stemmed Words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqJYAynk8uHa",
        "outputId": "ccad9e1c-118b-4ad1-b09a-2635045d506b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words: ['nltk', 'power', 'python', 'librari', 'natur', 'languag', 'process', '!', 'it', 'help', 'token', ',', 'stem', ',', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmitization(More accurate word normalization)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "print(\"Lemmatized Words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN6crbOO8uJv",
        "outputId": "4641bc5f-6f8b-4bc0-d600-b221e0eeb8a6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Words: ['NLTK', 'powerful', 'Python', 'library', 'natural', 'language', 'processing', '!', 'It', 'help', 'tokenization', ',', 'stemming', ',', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = pos_tag(filtered_tokens)\n",
        "print(\"POS Tags:\", pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqaC59rX8uL0",
        "outputId": "b8a745c2-5fb3-4e10-f78f-52da7bf01e4f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('NLTK', 'NNP'), ('powerful', 'JJ'), ('Python', 'NNP'), ('library', 'JJ'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('!', '.'), ('It', 'PRP'), ('helps', 'VBZ'), ('tokenization', 'NN'), (',', ','), ('stemming', 'VBG'), (',', ','), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Preprocessing Using Hugging Face"
      ],
      "metadata": {
        "id": "DNwAtNsuY8nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "fapSY0IBXK8F"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize text\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"Hugging Face Tokenized:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBXuLvAN9hVA",
        "outputId": "fea8bd59-f58b-4858-d9bf-f1a2c9cf5f57"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face Tokenized: ['nl', '##t', '##k', 'is', 'a', 'powerful', 'python', 'library', 'for', 'natural', 'language', 'processing', '!', 'it', 'helps', 'with', 'token', '##ization', ',', 'stemming', ',', 'and', 'more', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(text)\n",
        "print(\"Input IDs:\", input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8J-IOrrt9hXT",
        "outputId": "cff020eb-206e-4b7e-f3f8-346d4e8241f3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: [101, 17953, 2102, 2243, 2003, 1037, 3928, 18750, 3075, 2005, 3019, 2653, 6364, 999, 2009, 7126, 2007, 19204, 3989, 1010, 29217, 1010, 1998, 2062, 1012, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_text = tokenizer.decode(input_ids)\n",
        "print(\"Decoded Text:\", decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njxOpDIv9hZY",
        "outputId": "cd0485ba-d28d-4e08-8dac-eb8eb8b287de"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoded Text: [CLS] nltk is a powerful python library for natural language processing! it helps with tokenization, stemming, and more. [SEP]\n"
          ]
        }
      ]
    }
  ]
}